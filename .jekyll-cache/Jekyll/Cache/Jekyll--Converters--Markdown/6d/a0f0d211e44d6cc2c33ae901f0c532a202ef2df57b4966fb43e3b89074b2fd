I"*<p>블로그 포스트 다시 정리해서 써보기.</p>

<p>이번 레슨은 머신러닝에서의 가장 기초적인 개념들에 대해 학습한다고 한다.</p>

<h2 id="1-키워드">1. 키워드</h2>
<ul>
  <li>arrays, tensors, broadcasting</li>
  <li>SGD (stochastic gradient descent)</li>
  <li>loss function</li>
  <li>mini batch</li>
</ul>

<h2 id="2-머신러닝의-기초적인-개념">2. 머신러닝의 기초적인 개념</h2>

<h3 id="array--tensor">Array &amp; Tensor</h3>
<ul>
  <li>다차원으로 쌓은 상자들?의 이미지를 생각하면 될듯</li>
  <li>rank: 몇개의 축을 가진 데이터인지 말해준다. 예를 들어 rank가 2이면 텐서들로 이루어진 텐서 꼴인 것이다.</li>
  <li>shape: 각 축의 길이를 말해 준다. 예를 들어 [28, 28]은 28개의 row와 28개의 column을 가졌다는 것이고, [1010, 28, 28]은 28*28 정사각형이 1010개 쌓였다고 생각하면 된다.</li>
</ul>

<p><strong>Array 와 Tensor의 이점</strong></p>
<ul>
  <li>대체로 C로 컴파일된 메소드들을 가지고 있음: 빠르다</li>
</ul>

<p><strong>Numpy 와 Pytorch</strong></p>
<ul>
  <li>Numpy의 장점: 자유로운 형태를 가질 수 있다. 예를 들면 jagged array (array 안에 길이가 다른/type이 다른 array들이 있는 것). Pytorch는 숫자들로 구성된 길이가 같은 텐서들로 이루어져 있어야 한다. (그러니까 직사각형이나 큐브처럼 이루어져야 한다.)</li>
  <li>Pytroch의 장점: numpy에는 없는 GPU와 미분 (back propagation) 지원이 있다.</li>
</ul>

<h3 id="l1-norm-l2-norm">L1 Norm, L2 Norm</h3>
<ul>
  <li>L1 Norm: 편차의 절댓값의 평균: <code class="language-plaintext highlighter-rouge">(x-y).abs().mean()</code></li>
  <li>L2 Norm: 분산 (편차의 제곱의 평균에 루트를 씌운 것): <code class="language-plaintext highlighter-rouge">((x-y)**2).mean().sqrt().</code></li>
  <li>물론 pytorch에서 L1, L2 Norm 관련 메소드가 있다. (아래 유용한 pytorch 연산 정리 참고)</li>
  <li>L1 norm과 L2 norm의 차이: L2 norm은 L1에 비해 더 큰 오류에 민감하고 작은 오류에는 덜 민감하다</li>
</ul>

<h3 id="weights-bias-parameters">Weights, Bias, Parameters</h3>
<ul>
  <li>Parameters: 머신러닝 모델에서 트레이닝 하면서 변하는 것</li>
  <li>Weight: y = w * x + b 에서 w</li>
  <li>Bias: y = w * x + b 에서 b</li>
</ul>

<h3 id="matrix-multiplication">Matrix Multiplication</h3>
<ul>
  <li>y = w * x + b를 모든 트레이닝 데이터셋에 대해 for loop을 도는 대신, matrix multiplication을 이용하면 효율적이고 GPU를 이용할 수도 있어서 빠르다.</li>
  <li>python 그리고 pytorch에서 matrix multiplication은 @로 나타낸다.</li>
  <li><code class="language-plaintext highlighter-rouge">batch @ weight + bias</code></li>
</ul>

<h3 id="activation-function">Activation Function</h3>
<ul>
  <li>숫자들을 해당 분류에 해당할 확률? 같은 것으로 바꿔준다고 일단 이해했다.</li>
  <li>sigmoid의 경우, y값이 0과 1사이에서 왔다갔다하는 커브이다.</li>
</ul>

<h2 id="3-딥러닝-모델의-구조">3. 딥러닝 모델의 구조</h2>
<ol>
  <li>Parameter 초기화
    <ul>
      <li>일반적으로 그냥 random으로 설정한다.</li>
      <li>미리 훈련된 다른 모델에서 가져올 수도 있다 (transfer learning)</li>
    </ul>
  </li>
  <li>Predict</li>
  <li>Loss: 실제 레이블과 예측의 차이
    <ul>
      <li>미분가능한 함수여야 한다.</li>
    </ul>
  </li>
  <li>Gradient: 각각의 parameter을 어떤 방향으로 바꿀지 계산하기 위해</li>
  <li>Step(Gradient를 근거로)
    <ul>
      <li>step의 양은 gradient * lr (learning rate)</li>
      <li>pytorch에서는 step을 한 뒤에 <code class="language-plaintext highlighter-rouge">parameters.grad = None</code>을 해줘야 한다. ? 왜?</li>
    </ul>
  </li>
  <li>Repeat</li>
  <li>Stop (특정 기준에 의해)
    <ul>
      <li>너무 오래 걸려서</li>
      <li>모델이 충분히 좋아서 (metric이 특정 기준 이상)</li>
    </ul>
  </li>
</ol>

<h2 id="4-유용한-머신러닝-팁">4. 유용한 머신러닝 팁</h2>

<h3 id="baseline-model을-구축하라">Baseline Model을 구축하라</h3>
<ul>
  <li>종종 아주 멋진 모델보다 그냥 간단한 모델의 성능이 더 좋을 때가 있다.</li>
  <li>그러므로 Baseline model을 구축하고 그 이후의 모든 시도들이 baseline model보다 더 좋은 성능을 보이도록 해야 한다.</li>
  <li>baseline model을 구축할때는 정말 간단하고 짜기 쉬운 모델을 짜거나 기존에 다른 사람이 만든 모델을 이용할 수 있다.</li>
</ul>

<h3 id="중간중간-tensor의-shape를-확인해라">중간중간 tensor의 shape를 확인해라.</h3>
<ul>
  <li>shape가 이상해지면 좋을 것 없다.</li>
</ul>

<h3 id="broadcasting을-이용해라">Broadcasting을 이용해라.</h3>
<ul>
  <li>pytorch에서는 broadcasting을 이용하여 알아서 tensor의 차원을 확장시켜준다.</li>
  <li>대부분의 loop는 broadcasting으로 대신할 수 있다.</li>
  <li>Broadcasting은 C언어로/Cuda로 GPU에서 일어나므로 훨씬 빠르고, 메모리도 덜 사용한다.</li>
  <li>broadcasting을 이용하면 여러 shape의 텐서를 입력으로 받아도 둘다 작동하는 함수를 만들 수 있따.</li>
</ul>

<h2 id="5-유용한-pytorch-연산-정리">5. 유용한 Pytorch 연산 정리</h2>

<h3 id="기본-연산">기본 연산</h3>
<ul>
  <li>생성: <code class="language-plaintext highlighter-rouge">tensor()</code></li>
  <li>인덱싱: [1], [:, 1]. python에서의 인덱싱과 동일하다. 처음은 포함, 마지막은 미포함.</li>
  <li>사칙연산: <code class="language-plaintext highlighter-rouge">+</code>, <code class="language-plaintext highlighter-rouge">-</code> ,<code class="language-plaintext highlighter-rouge">*</code>, <code class="language-plaintext highlighter-rouge">/</code>. Broadcasting 또는 element-wise로 이루어진다.</li>
  <li>타입: <code class="language-plaintext highlighter-rouge">.type()</code>. 다만 알아서 type change도 해준다.</li>
  <li>복제: <code class="language-plaintext highlighter-rouge">.clone()</code></li>
</ul>

<h3 id="shpae-조정">shpae 조정</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">len(텐서)</code> 또는 <code class="language-plaintext highlighter-rouge">.ndim</code>: rank를 반환한다.</li>
  <li><code class="language-plaintext highlighter-rouge">.shape</code>: shape를 반환한다.</li>
  <li><code class="language-plaintext highlighter-rouge">stack</code>: shape가 같은 여러개의 tensor을 쌓아서 하나의 tensor로 만든다.</li>
  <li>타입 변화: 변화시키려고 하는 type을 메소드처럼 이용한다. ex) <code class="language-plaintext highlighter-rouge">.float()</code></li>
  <li><code class="language-plaintext highlighter-rouge">.view</code>: tensor의 shape를 바꾼다. 인자로는 목표로 하는 shape를 파라미터로 넣는다. <code class="language-plaintext highlighter-rouge">-1</code>을 인자로 넣으면 필요한 만큼 알아서라는 뜻이 된다. 예) <code class="language-plaintext highlighter-rouge">.view(-1, 28*28)</code></li>
  <li><code class="language-plaintext highlighter-rouge">.unsqueeze</code>: 하나의 차원을 추가한다.</li>
</ul>

<h3 id="루프-대체">루프 대체</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">.cat</code>: concatenate, 합치기</li>
  <li><code class="language-plaintext highlighter-rouge">.mean</code>: 평균을 반환한다. 만약 0,1,2등의 인덱스가 주어질 경우, 그 axis을 가로질러서 평균을 낸다. 예를 들어 shape가 [1010, 28, 28]인 텐서에서 <code class="language-plaintext highlighter-rouge">.mean(0)</code>을 하면 [28, 28] 텐서가 반환하는 식이다.</li>
  <li><code class="language-plaintext highlighter-rouge">.where(condition, x, y)</code>: condition이 성립하면 x, 그렇지 않으면 y를 가지는 텐서를 반환</li>
</ul>

<h3 id="l1-norm-l2-norm-1">L1 Norm, L2 Norm</h3>
<p>torch.nn.functional 모듈에서 구현되어 있다. Pytorch 팀에서는 torch.nn.functional을 F로 import하기를 권한다.</p>
<ul>
  <li>L1: <code class="language-plaintext highlighter-rouge">F.l1_loss()</code></li>
  <li>L2: <code class="language-plaintext highlighter-rouge">F.mse_loss().sqrt()</code> (정확히 말하자면 l2 norm이 아니라 mse가 구현되어 있다.)</li>
</ul>

<h3 id="미분">미분</h3>
<ul>
  <li>당연히 어떤 사이즈의 텐서이건 미분 할 수 있다.</li>
</ul>

<ol>
  <li>파라미터 텐서를 고른 후 .requires_grad_()로 설정한다.
    <ul>
      <li>텐서에 gradient function을 추가하는데, 여기에 우리가 어떤 연산을 할 때마다 이제 pytorch가 자동으로 함수를 추적해서 나중에 gradient를 계산할 수 있도록 한다.</li>
      <li>여기서 메소드가 _로 끝나는 것은 우리가 실제로 텐서를 바꾸고 싶다는 의미이다.</li>
      <li>예: <code class="language-plaintext highlighter-rouge">x = tensor(3.).requires_grad_()</code></li>
    </ul>
  </li>
  <li>필요한 연산을 해서 새로운 텐서에 저장한다.
    <ul>
      <li>예: <code class="language-plaintext highlighter-rouge">y = x ** 2</code></li>
    </ul>
  </li>
  <li>그 새로운 텐서에서 <code class="language-plaintext highlighter-rouge">.backward()</code> 메소드를 부른다. 그러면 pytorch가 gradient를 계산한다. 여기서 backward는 backpropagation을 의미한다.
    <ul>
      <li>예: <code class="language-plaintext highlighter-rouge">y.backward()</code></li>
    </ul>
  </li>
  <li>원래 데이터 텐서에서 <code class="language-plaintext highlighter-rouge">.grad</code> 속성을 확인한다.
    <ul>
      <li>예: <code class="language-plaintext highlighter-rouge">x.grad</code></li>
    </ul>
  </li>
</ol>

<p><strong>주의</strong></p>
<ul>
  <li>grad가 걸려있는 텐서에 계산을 수행할때, 모델에 포함되지 않는 연산이면 .data를 이용해 수정해야 한다. 그러지 않으면 pytorch는 그 연산도 모델의 일부라고 생각한다.</li>
</ul>

<h3 id="dataset-만들기">Dataset 만들기</h3>
<ul>
  <li>Pytorch에서 데이터셋은 아주 구체적이고 한정적인 의미를 가지고 있다.</li>
  <li>데이터셋은 인덱싱 되었을 때 (x,y) 튜플을 반환해야 한다.</li>
  <li>
    <p>구현: <code class="language-plaintext highlighter-rouge">list(zip(train_x,train_y))</code></p>
  </li>
  <li>array &amp; tensor</li>
  <li>python &amp; pytorch tensor programming trics with the baseline model</li>
  <li>L1, L2 norm</li>
  <li>broadcasting</li>
  <li>gradient descent</li>
  <li>cool pytorch features</li>
</ul>
:ET